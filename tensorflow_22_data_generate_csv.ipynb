{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "sys.version_info(major=3, minor=8, micro=2, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.1\n",
      "numpy 1.18.5\n",
      "pandas 1.0.4\n",
      "sklearn 0.23.1\n",
      "tensorflow 2.2.0\n",
      "tensorflow.keras 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 668)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定生成文件的目录\n",
    "output_dir = 'generate_csv'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# 定义函数实现将数据保存到对应csv文件中，传入文件目录，数据，文件前缀，\n",
    "# 默认头部为None，文件分割数为10\n",
    "def save_to_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
    "    \n",
    "    # 生成文件名\n",
    "    path_format = os.path.join(output_dir, '{}_{:02d}.csv')\n",
    "    filenames = []  # 用以存储子文件名\n",
    "    \n",
    "    # 分割数据\n",
    "    # len()得到数据长度，np.arange()生成对应长度的数组，n_parts为默认的分割数\n",
    "    # np.array_split()将生成数组按分割数进行分割，返回分割后的子数组对象\n",
    "    # enumerate()返回对象下标和对象，\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(len(data)), n_parts)):\n",
    "        \n",
    "        # 生成子文件名，拼接前缀和文件名\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        \n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header + '\\n')\n",
    "            \n",
    "            # 拿到数据切片中的各行\n",
    "            for row_index in row_indices:\n",
    "                # 遍历行中的数据转化为字符串后作为字符串列表\n",
    "                # 用逗号拼接内容，使用\\n换到下一行\n",
    "                f.write(','.join([repr(col) for col in data[row_index]]))\n",
    "                f.write('\\n')\n",
    "         \n",
    "    return filenames\n",
    "\n",
    "# 使用np.c_将特征值和目标值合并起来\n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "valid_data = np.c_[x_valid_scaled, y_valid]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "\n",
    "# 创建头部\n",
    "header_cols = housing.feature_names + ['MidianHouseValue']\n",
    "header_str = ','.join(header_cols)\n",
    "\n",
    "train_filenames = save_to_csv(output_dir, train_data, 'train', header_str, n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, 'valid', header_str, n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir, test_data, 'train', header_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train filenames:\n",
      "['generate_csv/train_00.csv',\n",
      " 'generate_csv/train_01.csv',\n",
      " 'generate_csv/train_02.csv',\n",
      " 'generate_csv/train_03.csv',\n",
      " 'generate_csv/train_04.csv',\n",
      " 'generate_csv/train_05.csv',\n",
      " 'generate_csv/train_06.csv',\n",
      " 'generate_csv/train_07.csv',\n",
      " 'generate_csv/train_08.csv',\n",
      " 'generate_csv/train_09.csv',\n",
      " 'generate_csv/train_10.csv',\n",
      " 'generate_csv/train_11.csv',\n",
      " 'generate_csv/train_12.csv',\n",
      " 'generate_csv/train_13.csv',\n",
      " 'generate_csv/train_14.csv',\n",
      " 'generate_csv/train_15.csv',\n",
      " 'generate_csv/train_16.csv',\n",
      " 'generate_csv/train_17.csv',\n",
      " 'generate_csv/train_18.csv',\n",
      " 'generate_csv/train_19.csv']\n",
      "valid filenames:\n",
      "['generate_csv/valid_00.csv',\n",
      " 'generate_csv/valid_01.csv',\n",
      " 'generate_csv/valid_02.csv',\n",
      " 'generate_csv/valid_03.csv',\n",
      " 'generate_csv/valid_04.csv',\n",
      " 'generate_csv/valid_05.csv',\n",
      " 'generate_csv/valid_06.csv',\n",
      " 'generate_csv/valid_07.csv',\n",
      " 'generate_csv/valid_08.csv',\n",
      " 'generate_csv/valid_09.csv']\n",
      "test filenames:\n",
      "['generate_csv/train_00.csv',\n",
      " 'generate_csv/train_01.csv',\n",
      " 'generate_csv/train_02.csv',\n",
      " 'generate_csv/train_03.csv',\n",
      " 'generate_csv/train_04.csv',\n",
      " 'generate_csv/train_05.csv',\n",
      " 'generate_csv/train_06.csv',\n",
      " 'generate_csv/train_07.csv',\n",
      " 'generate_csv/train_08.csv',\n",
      " 'generate_csv/train_09.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(\"train filenames:\")\n",
    "pprint.pprint(train_filenames)\n",
    "print(\"valid filenames:\")\n",
    "pprint.pprint(valid_filenames)\n",
    "print(\"test filenames:\")\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate_csv/train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_11.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 数据分割后，得到存储数据切片的不同文件\n",
    "# 从文件中获取数据\n",
    "\n",
    "# 使用.list_files()将文件名合并为一个文件名数据集dataset\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-0.3281387370556396,-1.0044649142435087,-0.17411778092728217,-0.05180209490726485,-0.17902174411178695,-0.07903731619060342,-1.306450703555216,1.3694277715423138,2.021'\n",
      "b'-0.19443506348560768,0.344485803429447,-0.2538104784225457,-0.22113402704060262,0.326656892234898,-0.04924278272520294,-1.3626632513495633,1.2498801887923283,1.265'\n",
      "b'-0.3930596464705963,-1.4012151253237897,1.2666081079428901,1.4297680870288512,-0.4375973818946468,-0.15971991501483038,-1.1331286811893087,1.21999329310483,2.813'\n",
      "b'-0.7283103876938144,0.5031858878615595,-0.31689488668120985,-0.15763087136164158,0.28606139961369814,0.07427588109429509,-0.13535595783961848,0.31342412391742397,0.54'\n",
      "b'-1.1295779700825632,1.1379862255900093,-0.6165760998210988,-0.21693328826697142,-0.545263688411742,-0.08322788611673292,-0.763062741543177,0.6122930807923913,1.897'\n",
      "b'-0.7762704485879827,1.3760363522381778,-0.46059818054710583,0.10980547641775501,-0.10400833383348292,0.03301655309248693,-0.7677471205260418,0.7218783649798798,1.568'\n",
      "b'-0.6950149373668509,-0.44901461873111514,0.07887256094973653,-0.05020102918515916,-0.6502824628013677,-0.0381532793753714,0.9701574821159052,-0.7077114787387299,1.611'\n",
      "b'0.41469424092868234,-0.13161444986689028,-0.24779166106262074,-0.11114631740740721,-0.27609792211900397,-0.09067344181337253,-0.8333284262861155,0.6023307822298942,3.169'\n",
      "b'0.49297508025539905,0.027085634565222165,0.5354914591850879,-0.18508799749486404,-0.5735040311047507,-0.047740066085009816,1.2559046000705107,-1.3851477809886619,1.951'\n",
      "b'-0.8643624864436477,-0.6077147031632276,-0.10212947036343568,0.09888714753275042,0.44667834868018447,-0.02416599209477711,-0.8145909103546664,1.0556153668235972,1.273'\n",
      "b'-1.2316561845646017,-0.13161444986689028,-0.36723295730923083,-0.0771395345046994,-0.4896655137348814,-0.13100536986582415,-0.7583783625603155,1.2598424873548253,1.262'\n",
      "b'1.1896223630369585,0.42383584564550325,0.18641567830225678,-0.30951953384713826,-0.11989352659830024,0.004283390916022378,-0.8426971842518384,0.6073119315111393,3.412'\n",
      "b'0.43932661170348924,-0.6870647453792837,-0.12735790870605512,-0.21411703363105955,1.6407153381689537,-0.02102687187338344,-0.7536939835774541,0.7517652606673779,1.851'\n",
      "b'-0.4498393485955747,-1.5599152097559021,-0.007076069247439563,-0.11035905796304585,-0.5761515632322202,-0.041542960637228336,-0.36489052799987653,0.7069349171361308,0.879'\n",
      "b'-0.11500610518209925,0.42383584564550325,-0.3590338523160335,-0.09795045983853962,-0.2946306470112909,0.01601889297190154,-0.6693751618859312,0.5774250358236411,1.846'\n"
     ]
    }
   ],
   "source": [
    "# 指定读取数目\n",
    "n_readers = 5\n",
    "\n",
    "# tf.data.TextLineDataset()，从另一个数据集中获取数据组成新的数据集\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "    cycle_length = n_readers\n",
    ")\n",
    "\n",
    "# .take()指定数据获取量\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=string, numpy=b' 2'>, <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: shape=(), dtype=string, numpy=b' 4'>]\n"
     ]
    }
   ],
   "source": [
    "# 使用tf.io.decode_csv()对字符串内容进行解析\n",
    "\n",
    "sample_str = '1, 2, 3, 4'\n",
    "\n",
    "# 指定解析内容类型，一一对应解析内容\n",
    "record_defaults = [\n",
    "    tf.constant(1, dtype=tf.int32),\n",
    "    'hello',\n",
    "    np.nan,\n",
    "    'hello',\n",
    "#     tf.constant([]),  # 默认为float32\n",
    "#     'hello',\n",
    "]\n",
    "\n",
    "\n",
    "parsed_fields = tf.io.decode_csv(sample_str, record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 4 fields but have 5 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# 无法解析空字符串\n",
    "try:\n",
    "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([-0.9868721 ,  0.8328631 , -0.18684709, -0.1488895 , -0.45323023,\n",
       "        -0.11504996,  1.6730974 , -0.74654967], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.138], dtype=float32)>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_csv_line(line, n_fields=9):\n",
    "    \n",
    "    # 根据长度定义默认类型均为tf.constant(np.nan)\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    \n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    \n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "parse_csv_line(b'-0.9868720801669367,0.832863080552588,-0.18684708416901633,-0.14888949288707784,-0.4532302419670616,-0.11504995754593579,1.6730974284189664,-0.7465496877362412,1.138',\n",
    "               n_fields=9)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
      "array([[-0.39305964, -1.4012151 ,  1.2666081 ,  1.4297681 , -0.4375974 ,\n",
      "        -0.15971991, -1.1331286 ,  1.2199932 ],\n",
      "       [-0.32813874, -1.0044649 , -0.17411777, -0.0518021 , -0.17902175,\n",
      "        -0.07903732, -1.3064507 ,  1.3694278 ],\n",
      "       [-0.15503371,  0.26513577,  0.18460795, -0.15213478, -0.49319556,\n",
      "        -0.07645705,  2.4082618 , -2.26183   ]], dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[2.813],\n",
      "       [2.021],\n",
      "       [0.928]], dtype=float32)>\n",
      "x:\n",
      "<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
      "array([[-0.19443506,  0.3444858 , -0.25381047, -0.22113402,  0.32665688,\n",
      "        -0.04924278, -1.3626633 ,  1.2498802 ],\n",
      "       [ 0.46197587, -0.29031453,  0.19074658, -0.20333153, -0.29021809,\n",
      "         0.01428765, -0.92233163,  0.81153905],\n",
      "       [ 0.41469425, -0.13161445, -0.24779166, -0.11114632, -0.27609792,\n",
      "        -0.09067344, -0.8333284 ,  0.6023308 ]], dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[1.265],\n",
      "       [2.307],\n",
      "       [3.169]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 读取数据的完整流程\n",
    "\n",
    "# 传入的参数：文件名，读取数，每批次数据量，进行解析字段数，混洗缓存的大小\n",
    "def csv_read_dataset(filenames, n_readers=5,\n",
    "                    batch_size=32, n_parse_threads=5,\n",
    "                    shuffle_buffer_size=10000):\n",
    "    \n",
    "    # 将文件名组成一个新的数据集\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    \n",
    "    # 设置数据集可重复获取\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # 提取文件中的内容作为新的数据集\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    \n",
    "    # .map()对数据集进行映射，应用函数\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    \n",
    "    # 使用.batch()对数据集进行分批\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "train_set = csv_read_dataset(train_filenames, batch_size=3)\n",
    "\n",
    "for x_batch, y_batch in train_set.take(2):\n",
    "    print('x:')\n",
    "    pprint.pprint(x_batch)\n",
    "    print('y:')\n",
    "    pprint.pprint(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = csv_read_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_read_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_read_dataset(test_filenames,\n",
    "                              batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.8575 - val_loss: 0.5868\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.5187 - val_loss: 0.9330\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4703 - val_loss: 1.1827\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4437 - val_loss: 1.2898\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4328 - val_loss: 1.2119\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4169 - val_loss: 1.0973\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-2)]\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    validation_data = valid_set,\n",
    "                    steps_per_epoch = 11160 // batch_size, #每步训练集数目\n",
    "                    validation_steps = 3870 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4229345917701721"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps = 5160 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
